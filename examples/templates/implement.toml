# ═══════════════════════════════════════════════════════════════════════════════
# IMPLEMENT TEMPLATE
# ═══════════════════════════════════════════════════════════════════════════════
# Test-Driven Development (TDD) workflow for implementing a single task.
#
# Flow:
#   1. Load context - understand the task and codebase
#   2. Write tests - define expected behavior as failing tests
#   3. Verify fail - ensure tests fail for the right reasons
#   4. Implement - write minimum code to pass tests
#   5. Verify pass - ensure all tests pass
#   6. Review - self-review for quality
#   7. Commit - create descriptive commit
# ═══════════════════════════════════════════════════════════════════════════════

[meta]
name = "implement"
version = "1.0.0"
description = "TDD implementation workflow for a single task"
author = "meow-stack"

# This should complete in one Claude session
fits_in_context = true
estimated_minutes = 30

# Error handling
on_error = "inject-gate"
max_retries = 2

# ───────────────────────────────────────────────────────────────────────────────
# Variables
# ───────────────────────────────────────────────────────────────────────────────

[variables]
task_id = { required = true, description = "The task bead ID to implement" }
epic_id = { required = false, description = "Parent epic ID if applicable" }
test_framework = { required = false, default = "auto", description = "Test framework (auto-detect, pytest, jest, go, etc.)" }
skip_tests = { required = false, default = false, type = "bool", description = "Skip TDD steps (not recommended)" }

# ───────────────────────────────────────────────────────────────────────────────
# Steps
# ───────────────────────────────────────────────────────────────────────────────

[[steps]]
id = "load-context"
description = "Load relevant files and understand the task"
instructions = """
## Task Context Loading

Read the task description:
```bash
bd show {{task_id}}
```

Understand:
1. **What** needs to be implemented (requirements)
2. **Where** in the codebase (relevant files)
3. **How** it fits with existing code (patterns, conventions)
4. **Why** it's needed (context, dependencies)

### Actions:
- Read the task description, acceptance criteria, and design notes
- Identify source files that will need modification
- Identify related files for context
- Note any dependencies or blockers

### Output:
Document your understanding in this step's notes:
- Key files to modify
- Approach summary
- Any concerns or questions
"""

[[steps]]
id = "write-tests"
description = "Write failing tests that define success criteria"
needs = ["load-context"]
condition = "not {{skip_tests}}"
instructions = """
## Write Failing Tests

Based on the task requirements, write tests that:

### Test Coverage:
1. **Happy path** - Main functionality works correctly
2. **Edge cases** - Boundary conditions, empty inputs, etc.
3. **Error cases** - Invalid inputs, error handling
4. **Integration points** - Interaction with other components

### Guidelines:
- Use {{test_framework}} conventions
- Follow existing test patterns in the codebase
- Tests should be clear and self-documenting
- Each test should test ONE thing

### Test Structure:
```
Given: [initial state/setup]
When: [action being tested]
Then: [expected outcome]
```

### Actions:
- Create new test file(s) if needed
- Add test cases covering requirements
- Ensure tests are runnable (imports, fixtures, etc.)

### DO NOT:
- Implement the actual feature yet
- Write tests that pass without implementation
"""

[[steps]]
id = "verify-fail"
description = "Run tests and verify they fail as expected"
needs = ["write-tests"]
condition = "not {{skip_tests}}"
instructions = """
## Verify Tests Fail

Run the test suite and confirm new tests fail appropriately.

### Run Tests:
```bash
# Auto-detect and run
{{test_framework}} -v  # or appropriate command
```

### Verify:
1. ✓ New tests FAIL (expected - no implementation yet)
2. ✓ Existing tests PASS (no regressions)
3. ✓ Failures are for the RIGHT reasons (not import errors, typos, etc.)

### Red Flags:
- Tests pass without implementation → tests don't test anything
- Tests fail with import errors → fix imports first
- Existing tests fail → investigate before proceeding

### If Issues:
Fix test setup issues before proceeding. Do not continue with broken tests.
"""
validation = "new_tests_fail and existing_tests_pass"

[[steps]]
id = "implement"
description = "Write code to make tests pass"
needs = ["verify-fail"]
instructions = """
## Implement the Feature

Write the minimum code necessary to make all tests pass.

### Guidelines:
1. **Minimum viable** - Don't over-engineer
2. **Follow patterns** - Match existing codebase style
3. **Keep it simple** - Complexity is the enemy
4. **One thing at a time** - Small, focused changes

### Process:
1. Start with the simplest failing test
2. Write just enough code to pass it
3. Move to the next failing test
4. Refactor if needed (tests protect you)

### Quality Checklist:
- [ ] Code follows project conventions
- [ ] No obvious security issues
- [ ] Error cases handled appropriately
- [ ] No hardcoded values that should be config
- [ ] No debug code or console.logs left behind

### DO NOT:
- Add features not covered by tests
- Refactor unrelated code
- "Improve" things outside scope
"""

[[steps]]
id = "verify-pass"
description = "Run tests and verify they all pass"
needs = ["implement"]
instructions = """
## Verify All Tests Pass

Run the full test suite and confirm everything passes.

### Run Tests:
```bash
{{test_framework}} -v  # Full test suite
```

### Verify:
1. ✓ All new tests PASS
2. ✓ All existing tests PASS
3. ✓ No warnings or deprecations introduced
4. ✓ No flaky tests

### If Tests Fail:
- Read the error message carefully
- Fix the implementation (not the test, unless test is wrong)
- Run again until green

### Coverage (optional):
```bash
{{test_framework}} --cov  # If coverage is set up
```
"""
validation = "all_tests_pass"

[[steps]]
id = "review"
description = "Self-review the implementation"
needs = ["verify-pass"]
instructions = """
## Self-Review

Review your changes before committing.

### Code Quality:
- [ ] Code is readable and self-documenting
- [ ] Variable/function names are clear
- [ ] No unnecessary complexity
- [ ] DRY - no copy-paste code
- [ ] Comments only where logic is non-obvious

### Error Handling:
- [ ] Errors are caught and handled appropriately
- [ ] Error messages are helpful
- [ ] No silent failures
- [ ] Resources are cleaned up (files, connections)

### Performance:
- [ ] No obvious N+1 queries
- [ ] No unnecessary loops or computations
- [ ] Large data handled appropriately

### Security:
- [ ] No hardcoded secrets
- [ ] Input is validated/sanitized
- [ ] No SQL injection / XSS / etc.
- [ ] Permissions checked where needed

### Review Diff:
```bash
git diff
```

### Make Refinements:
If you find issues, fix them now. Re-run tests after any changes.
"""

[[steps]]
id = "commit"
description = "Commit changes with a descriptive message"
needs = ["review"]
instructions = """
## Create Commit

Create a well-structured commit for this task.

### Stage Changes:
```bash
git add -A  # Or specific files
git status  # Verify what's staged
```

### Commit Message Format:
Follow the project's conventions. If none specified, use:

```
<type>(<scope>): <subject>

<body>

Refs: {{task_id}}
```

Types: feat, fix, refactor, test, docs, chore

### Example:
```
feat(auth): add user registration endpoint

- Add POST /api/register endpoint
- Implement email validation
- Add password strength requirements

Refs: {{task_id}}
```

### Create Commit:
```bash
git commit -m "..."
```

### Verify:
```bash
git log -1  # Check the commit
git status  # Should be clean
```
"""
